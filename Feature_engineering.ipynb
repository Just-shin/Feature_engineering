{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Repeated question is not answer.)\n",
    "\n",
    "### 1. ***What is a Parameter ?***\n",
    "*Answer-*\n",
    "\n",
    "    In the context of Machine Learning, a parameter is a internal variables or setting within an algorithm that can be adjusted to control the model's behavior. It's a value that the algorithm learns or tunes during the training process to optimize its performance(how input features are transformed into predictions). \n",
    "\n",
    "*   Key examples:\n",
    "\n",
    "*   Linear Regression:\n",
    "    Weights (coefficients) for each feature.\n",
    "    Bias (intercept) term.\n",
    "\n",
    "    eg- y = w1* x1 + w2*x2 + b\n",
    "\n",
    "*   Neural Networks:\n",
    "\n",
    "    Weight matrices between layers.\n",
    "    Bias vectors.\n",
    "    Number of layers/neurons (hyperparameters).\n",
    "\n",
    "\n",
    "*   Decision Trees:\n",
    "\n",
    "    Split points for each node.\n",
    "    Feature selection at each split.\n",
    "\n",
    "\n",
    "*   Support Vector Machines:\n",
    "\n",
    "    Support vector coefficients.\n",
    "    Kernel parameters.\n",
    "\n",
    "*Parameters are adjusted during training to minimize the loss function. For example, in a simple linear regression predicting house prices:*\n",
    "* price = w1 * square_feet + w2 * bedrooms + b. Here w1, w2, and b are parameters learned from the data. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ***What is correlation ? What does negetive correlation mean ?***\n",
    "*Answer-*\n",
    "\n",
    "    Correlation measures the statistical relationship between two or more variables(-1 to +1). It indicates how strongly the values of one variable are associated with the values of another.\n",
    "\n",
    "\n",
    "• +1: Perfect positive correlation (as X increases, Y increases)\n",
    "Example: Height and weight generally increase together\n",
    "\n",
    "• -1: Perfect negative correlation (as X increases, Y decreases)\n",
    "Example: Temperature and heating bill typically move in opposite directions\n",
    "\n",
    "• 0: No correlation (variables move independently)\n",
    "Example: Shoe size and test scores have no meaningful relationship\n",
    "\n",
    "Key points:\n",
    "- Correlation doesn't imply causation\n",
    "- Can be calculated using Pearson's coefficient (linear) or Spearman's rank (non-linear)\n",
    "- Useful for feature selection in machine learning\n",
    "\n",
    "Python calculation:\n",
    "```python\n",
    "import pandas as pd\n",
    "correlation = df['variable1'].corr(df['variable2'])\n",
    "```\n",
    "\n",
    "    Negative correlation means that as the value of one variable increases, the value of the other variable tends to decrease. In other words, they move in opposite directions. A perfect negative correlation has a correlation coefficient of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ***Define Machine Learning. What are the main components in Machine Learning?***\n",
    "*Answer-*\n",
    "\n",
    "    Machine Learning is a subfield/subset of Artificial Intelligence(AI) that focuses on the development of algorithms and statistical models that allow computer systems to learn pattern from data and improve their performance without being explicitly programmed.\n",
    "\n",
    "* Main Components:-\n",
    "\n",
    "1. Data-\n",
    "\n",
    "- Training data.\n",
    "- Validation data.\n",
    "- Test data.\n",
    "- Features (input variables).\n",
    "- Labels/Targets (output variables).\n",
    "\n",
    "2. Algorithm/Model-\n",
    "\n",
    "- Learning method (supervised, unsupervised, reinforcement).\n",
    "- Model architecture.\n",
    "- Parameters and hyperparameters.\n",
    "\n",
    "3. Training Process-\n",
    "\n",
    "- Loss function.\n",
    "- Optimization method.\n",
    "- Evaluation metrics.\n",
    "- Training iterations.\n",
    "\n",
    "4. Feature Engineering-\n",
    "\n",
    "- Feature selection.\n",
    "- Feature scaling.\n",
    "- Dimensionality reduction.\n",
    "- Data preprocessing.\n",
    "\n",
    "5. Model Evaluation-\n",
    "\n",
    "- Performance metrics.\n",
    "- Cross-validation.\n",
    "- Model validation techniques.\n",
    "- Error analysis.\n",
    "\n",
    "6. Deployment Infrastructure-\n",
    "\n",
    "- Model serving.\n",
    "- Monitoring systems.\n",
    "- Update mechanisms.\n",
    "\n",
    "Each component plays a crucial role in building effective machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ***How does loss value help in determining whether the model is good or not?***\n",
    "*Answer-*\n",
    "\n",
    "    The loss value is a key metric used to evaluate how well a machine learning model is performing. It measures the difference between the predicted outputs of the model and the actual target values. A lower loss value typically indicates that the model's predictions are closer to the actual outcomes, while a higher loss value suggests that the model's predictions are inaccurate.\n",
    "\n",
    "### How Loss Helps in Determining Model Performance:\n",
    "\n",
    "1. **Indicates Prediction Accuracy**:\n",
    "   - **High loss**: If the loss value is high, it means that the model's predictions are far from the actual values, which indicates poor model performance.\n",
    "   - **Low loss**: A lower loss indicates that the model's predictions are close to the true values, which suggests the model is performing better.\n",
    "\n",
    "2. **Optimization Process**:\n",
    "   - During the training of a machine learning model, the objective is often to minimize the loss function. The model learns to adjust its parameters (weights) to reduce this loss over time.\n",
    "   - As the training progresses and the loss decreases, the model is expected to improve, becoming more accurate in its predictions.\n",
    "\n",
    "3. **Comparison Between Models**:\n",
    "   - When comparing different models or configurations (e.g., different architectures or hyperparameters), the model with the lower loss on a validation set is generally considered better, as it demonstrates better generalization to unseen data.\n",
    "\n",
    "4. **Overfitting and Underfitting Detection**:\n",
    "   - **Overfitting**: If a model's loss on the training set is very low but the loss on the validation set is high, it suggests overfitting—meaning the model has learned the training data too well, including its noise, but doesn't generalize well to new data.\n",
    "   - **Underfitting**: If both training and validation losses are high, it suggests underfitting—meaning the model is too simple or not trained enough to capture the patterns in the data.\n",
    "\n",
    "5. **Different Loss Functions**:\n",
    "   - Different types of problems use different loss functions. For example:\n",
    "     - In **regression** problems, Mean Squared Error (MSE) or Mean Absolute Error (MAE) is often used.\n",
    "     - In **classification** problems, Cross-Entropy Loss or Binary Cross-Entropy is common.\n",
    "     - The type of loss function used helps determine the right metric for evaluating the model's effectiveness in the context of the problem.\n",
    "\n",
    "6. **Tracking Training Progress**:\n",
    "   - The loss value can be plotted over epochs during training to track how well the model is learning. A smooth, decreasing loss curve indicates good learning, whereas erratic or stagnant loss curves may indicate issues with the training process or model architecture.\n",
    "\n",
    " In summary, the loss value serves as a key indicator of how well the model is learning and how well it generalizes to unseen data. However, it's important to consider additional metrics, such as accuracy, precision, recall, or F1-score, depending on the problem type, for a more comprehensive assessment of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ***What are continuous and categorical variables?***\n",
    "*Answer-*\n",
    "\n",
    "### Continuous and Categorical Variables:\n",
    "\n",
    "In statistics and machine learning, variables are classified into two broad categories: **continuous variables** and **categorical variables**. They represent different types of data, and understanding the distinction between them is crucial for selecting the right analytical techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Continuous Variables**:\n",
    "\n",
    "- **Definition**: Continuous variables are numerical values that can take any value within a certain range or interval. They can represent quantities that are measured on a continuous scale, and the possible values are infinite, often including fractions or decimals.\n",
    "  \n",
    "- **Examples**:\n",
    "  - **Height** (e.g., 5.5 ft, 5.55 ft, 5.555 ft)\n",
    "  - **Weight** (e.g., 70.5 kg, 70.55 kg)\n",
    "  - **Temperature** (e.g., 22.5°C, 22.55°C)\n",
    "  - **Age** (e.g., 25.5 years, 25.75 years)\n",
    "\n",
    "- **Characteristics**:\n",
    "  - They can be divided into smaller units and represent more precise measurements.\n",
    "  - Continuous variables are typically associated with **interval** or **ratio scales** in measurement.\n",
    "  - These variables allow for a wide range of statistical operations (like mean, standard deviation, regression) because they are measured on a continuous scale.\n",
    "\n",
    "- **Applications**: Continuous variables are often used in scientific measurements, economics, engineering, and other fields where precision is needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Categorical Variables**:\n",
    "\n",
    "- **Definition**: Categorical variables represent data that can be divided into distinct categories or groups. The values are qualitative rather than quantitative, and they describe categories without inherent numerical relationships between them.\n",
    "\n",
    "- **Examples**:\n",
    "  - **Gender** (e.g., Male, Female)\n",
    "  - **Color** (e.g., Red, Blue, Green)\n",
    "  - **Marital Status** (e.g., Single, Married, Divorced)\n",
    "  - **Educational Level** (e.g., High School, Undergraduate, Graduate)\n",
    "\n",
    "- **Types of Categorical Variables**:\n",
    "  - **Nominal**: Categories that do not have any inherent order or ranking.\n",
    "    - Example: Color (Red, Blue, Green) — no natural ranking.\n",
    "  - **Ordinal**: Categories with a meaningful order or ranking, but the differences between the categories are not numerically significant.\n",
    "    - Example: Education level (High School < Undergraduate < Graduate) — there's a natural ranking, but the difference between each level isn't quantifiable.\n",
    "\n",
    "- **Characteristics**:\n",
    "  - They represent **qualitative** attributes rather than quantities.\n",
    "  - Arithmetic operations (like addition, subtraction) don’t apply directly to categorical data.\n",
    "  - Statistical methods for categorical data often involve counting occurrences (e.g., frequency) or using measures like mode.\n",
    "\n",
    "- **Applications**: Categorical variables are commonly found in surveys, demographics, social science studies, and other areas where grouping and classification are important.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature                   | **Continuous Variables**                          | **Categorical Variables**                           |\n",
    "|---------------------------|---------------------------------------------------|-----------------------------------------------------|\n",
    "| **Type of Data**           | Numeric (quantitative)                           | Non-numeric (qualitative)                           |\n",
    "| **Possible Values**        | Infinite, within a range (including decimals)     | Discrete categories, with no numeric value          |\n",
    "| **Measurement Scale**      | Interval or ratio scale                          | Nominal or ordinal scale                            |\n",
    "| **Examples**               | Height, Weight, Temperature, Age                  | Gender, Marital Status, Color, Education Level     |\n",
    "| **Mathematical Operations**| Can apply arithmetic operations (mean, variance) | Cannot apply arithmetic operations directly          |\n",
    "| **Data Representation**    | Real numbers (decimals, fractions)               | Categories or labels (text or numbers as labels)    |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Continuous variables** are numeric and can take on any value within a range, allowing for precise measurements and complex statistical analysis.\n",
    "- **Categorical variables** are non-numeric and represent distinct categories or groups, and statistical analysis often involves counting or comparing frequencies.\n",
    "\n",
    "Understanding the distinction between these types of variables helps in choosing appropriate statistical methods and tools for analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ***How do we handle categorical variables in Machine Learning? What are the common techniques?***\n",
    "*Answer-*\n",
    "\n",
    "    Handling categorical variables is an essential part of preprocessing data for machine learning. Most machine learning algorithms require numeric input, so categorical variables (which are non-numeric) need to be transformed into numerical representations. There are several techniques for handling categorical data, and the choice of method depends on the nature of the data and the model being used.\n",
    "\n",
    "### Common Techniques for Handling Categorical Variables:\n",
    "\n",
    "#### 1. **Label Encoding**:\n",
    "\n",
    "- **Description**: Label Encoding involves converting each category into a unique integer value. For example, if we have a categorical variable like \"Color\" with categories: \"Red\", \"Green\", \"Blue\", we could encode them as:  \n",
    "  - Red → 0  \n",
    "  - Green → 1  \n",
    "  - Blue → 2\n",
    "\n",
    "- **When to Use**: Label Encoding works well when the categorical variable has an **ordinal relationship** (i.e., the categories have a meaningful order). For example, \"Education Level\" (High School < Undergraduate < Graduate) could be encoded numerically because the order matters.\n",
    "\n",
    "- **Limitations**: \n",
    "  - Label Encoding may introduce unintended ordinal relationships in cases where there is no inherent order, causing the model to incorrectly interpret the data.\n",
    "  - It’s not ideal for nominal categorical variables (without order), like \"Color\" or \"City\", since it can create false assumptions about the relationships between categories.\n",
    "\n",
    "#### 2. **One-Hot Encoding**:\n",
    "\n",
    "- **Description**: One-Hot Encoding converts each category into a separate binary column (0 or 1). For example, if you have the \"Color\" variable with three categories: \"Red\", \"Green\", and \"Blue\", one-hot encoding would create three columns, one for each color, and assign a 1 for the respective color and 0 for others.\n",
    "  - Red → (1, 0, 0)\n",
    "  - Green → (0, 1, 0)\n",
    "  - Blue → (0, 0, 1)\n",
    "\n",
    "- **When to Use**: One-Hot Encoding is used for **nominal** categorical variables (those without a meaningful order), where each category is treated equally. It's particularly useful when the categorical variable has no ordinal meaning, such as \"City\" or \"Animal Type.\"\n",
    "\n",
    "- **Limitations**:\n",
    "  - It can increase the dimensionality of the dataset significantly, especially when there are many unique categories (e.g., if you have 1000 unique values, it will create 1000 columns).\n",
    "  - It may not be efficient for categorical variables with many levels or for algorithms that don't handle sparse data well (e.g., decision trees).\n",
    "\n",
    "#### 3. **Binary Encoding**:\n",
    "\n",
    "- **Description**: Binary Encoding is a hybrid method that combines the properties of **label encoding** and **one-hot encoding**. First, each category is assigned an integer (as in label encoding), and then the integer is converted to binary code. For example:\n",
    "  - \"Red\" → 1 → (0, 0, 1) (binary representation)\n",
    "  - \"Green\" → 2 → (0, 1, 0)\n",
    "  - \"Blue\" → 3 → (0, 1, 1)\n",
    "\n",
    "- **When to Use**: Binary Encoding is effective when dealing with high cardinality categorical variables (i.e., variables with a large number of categories). It reduces the dimensionality compared to one-hot encoding.\n",
    "\n",
    "- **Limitations**: Binary encoding can introduce relationships between values (because the binary representation of integers can imply some proximity or order), which may not be desirable for all datasets.\n",
    "\n",
    "#### 4. **Target Encoding (Mean Encoding)**:\n",
    "\n",
    "- **Description**: Target encoding replaces each category with the mean of the target variable (i.e., the dependent variable) for that category. For example, if the target variable is \"Price,\" the \"Color\" feature might be replaced with the average price for each color.\n",
    "  - Red → 20,000 (average price for Red cars)\n",
    "  - Green → 18,500 (average price for Green cars)\n",
    "\n",
    "- **When to Use**: Target encoding is often used for **ordinal** or **nominal** categorical variables, especially in cases where there is a strong correlation between the category and the target variable.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Can lead to **overfitting** if the dataset is small or if there is leakage from the target variable.\n",
    "  - Requires careful handling to avoid introducing bias, especially in cross-validation and testing phases (e.g., encoding using only training data).\n",
    "\n",
    "#### 5. **Frequency or Count Encoding**:\n",
    "\n",
    "- **Description**: This technique replaces each category with the **frequency** or **count** of its occurrences in the dataset. For example, if \"Red\" appears 100 times in the data, \"Red\" would be encoded as 100.\n",
    "\n",
    "- **When to Use**: Frequency or count encoding can be useful when there is a high cardinality of categories, and there is an implicit relationship between the frequency of the category and the target variable.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Like target encoding, it may introduce some correlation between the feature and the target if the category frequency correlates with the target.\n",
    "  - This method may not be effective in capturing non-linear relationships or categorical information that does not depend on frequency.\n",
    "\n",
    "#### 6. **Embedding Layers (for Deep Learning)**:\n",
    "\n",
    "- **Description**: In deep learning, categorical variables can be encoded using **embedding layers**. This approach involves representing categories as dense vectors in a lower-dimensional space. These embeddings are learned during training, allowing the model to discover meaningful relationships between categories.\n",
    "\n",
    "- **When to Use**: Embedding layers are used when you have categorical variables with **many distinct categories** (e.g., words in natural language, product IDs, or other high-cardinality features) and are typically used in neural network architectures like deep learning models.\n",
    "\n",
    "- **Limitations**: Requires a more complex setup and is typically used for deep learning models, not simpler models like decision trees or linear models.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Techniques:\n",
    "\n",
    "| Technique           | Best For                                         | Pros                                   | Cons                                   |\n",
    "|---------------------|--------------------------------------------------|----------------------------------------|----------------------------------------|\n",
    "| **Label Encoding**   | Ordinal variables                                | Simple and fast                       | Can create unintended ordinal relationships in nominal data |\n",
    "| **One-Hot Encoding** | Nominal variables with low cardinality          | No assumptions about data              | High dimensionality for high cardinality |\n",
    "| **Binary Encoding**  | High cardinality nominal variables               | Reduced dimensionality compared to one-hot encoding | Can introduce unintended relationships |\n",
    "| **Target Encoding**  | Ordinal or nominal variables with a strong relationship to the target | Can be more informative for certain models | Risk of overfitting or data leakage |\n",
    "| **Frequency Encoding** | High cardinality categorical variables          | Simple and efficient                   | May not capture non-linear relationships |\n",
    "| **Embedding Layers** | High cardinality categorical variables (especially for deep learning) | Captures relationships automatically | Requires deep learning models, complexity in training |\n",
    "\n",
    "### Choosing the Right Technique:\n",
    "\n",
    "- **Low Cardinality**: If the categorical variable has few unique categories, **One-Hot Encoding** or **Label Encoding** is often the best choice.\n",
    "- **High Cardinality**: For variables with many categories, consider **Binary Encoding**, **Frequency Encoding**, or **Target Encoding** to avoid a high-dimensional feature space.\n",
    "- **Ordinal Variables**: Use **Label Encoding** if there is a natural order, or **Target Encoding** if the target variable is closely related to the categories.\n",
    "- **Deep Learning Models**: If using deep learning models, **Embedding Layers** can be an effective way to represent categorical variables.\n",
    "\n",
    "Choosing the right technique for encoding categorical variables depends on the model you're using and the nature of the data. Each method has its trade-offs, so it's important to consider the specifics of your data and problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ***What do you mean by training and testing a dataset?***\n",
    "*Answer-*\n",
    "\n",
    "    Training Dataset: A portion of the data used to train the model, allowing it to learn patterns and adjust its parameters.\n",
    "\n",
    "    Testing Dataset: A separate portion of the data used to evaluate the model's performance on unseen data. This helps assess the model's ability to generalize and make accurate predictions on new, unknown examples.\n",
    "\n",
    "    Training and testing a dataset are key steps in developing and evaluating machine learning models. \n",
    "\n",
    "### **1. Training a Dataset**\n",
    "- **Purpose**: To teach the machine learning model by exposing it to labeled examples.\n",
    "- **Process**: \n",
    "  - The dataset (called the **training set**) contains input data and corresponding target outputs (labels or values).\n",
    "  - The model learns to map inputs to outputs by minimizing the error between the predicted outputs and the actual labels.\n",
    "  - Algorithms like gradient descent adjust the model's parameters (weights) to improve its accuracy on the training data.\n",
    "\n",
    "- **Example**: \n",
    "  If you're training a model to classify images of cats and dogs, the training set would consist of images labeled as \"cat\" or \"dog.\" The model learns patterns from these labeled images.\n",
    "\n",
    "### **2. Testing a Dataset**\n",
    "- **Purpose**: To evaluate the performance and generalization ability of the model on unseen data.\n",
    "- **Process**:\n",
    "  - The dataset (called the **test set**) is kept separate from the training data.\n",
    "  - After training, the model is tested on the test set to measure its accuracy, precision, recall, or other metrics.\n",
    "  - The model predictions on the test data are compared to the actual labels to determine how well it performs on data it hasn't seen before.\n",
    "\n",
    "- **Example**:\n",
    "  After training the cat-dog classifier, you test it with a separate set of images that the model hasn’t encountered during training. The test results indicate how well the model would perform in real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Split a Dataset into Training and Testing?**\n",
    "- **Avoid Overfitting**: If a model is evaluated only on the data it was trained on, it might perform well but fail on new data (overfitting).\n",
    "- **Real-world Performance**: Testing simulates how the model will behave in real-world applications with new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Practices**\n",
    "- **Data Splits**: Typically, 70-80% of the data is used for training, and 20-30% is reserved for testing.\n",
    "- **Validation Set**: Sometimes, a separate validation set is used during training to tune hyperparameters and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ***What is sklearn.preprocessing?***\n",
    "*Answer-*\n",
    "\n",
    "`sklearn.preprocessing` is a module in the Scikit-learn library that provides various utilities to preprocess and transform raw data into a format that is suitable for machine learning algorithms. Proper preprocessing is essential to improve model performance and efficiency. The module includes functions for:\n",
    "\n",
    "### Key Features of `sklearn.preprocessing`:\n",
    "\n",
    "1. **Scaling and Normalization:**\n",
    "   - Ensures that features have the same scale or distribution, which is particularly important for algorithms sensitive to feature magnitudes, like gradient descent.\n",
    "   - **Examples:**\n",
    "     - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
    "     - `MinMaxScaler`: Scales features to a specified range, often [0, 1].\n",
    "     - `Normalizer`: Scales input vectors individually to have unit norm (useful for sparse data).\n",
    "\n",
    "2. **Encoding Categorical Data:**\n",
    "   - Converts categorical variables into numerical formats so that machine learning algorithms can process them.\n",
    "   - **Examples:**\n",
    "     - `LabelEncoder`: Converts each category into an integer value.\n",
    "     - `OneHotEncoder`: Creates binary (one-hot) encoded columns for each category.\n",
    "\n",
    "3. **Imputation of Missing Data:**\n",
    "   - Handles missing values by filling them with a specific value, such as the mean, median, or mode.\n",
    "   - **Example:**\n",
    "     - `SimpleImputer`: Replaces missing values using strategies like mean, median, or a constant.\n",
    "\n",
    "4. **Binarization:**\n",
    "   - Converts numerical data into binary format based on a threshold.\n",
    "   - **Example:**\n",
    "     - `Binarizer`: Transforms data by thresholding.\n",
    "\n",
    "5. **Polynomial Feature Generation:**\n",
    "   - Generates new features representing polynomial combinations of the original features.\n",
    "   - **Example:**\n",
    "     - `PolynomialFeatures`: Adds interaction and polynomial terms for a given degree.\n",
    "\n",
    "6. **Custom Transformers:**\n",
    "   - Allows users to create custom transformations using the `FunctionTransformer`.\n",
    "\n",
    "This module simplifies preprocessing steps, making it easier to prepare data for modeling efficiently and accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "Encoded Data:\n",
      " [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Encoding categorical data\n",
    "encoder = OneHotEncoder()\n",
    "categories = np.array([['cat'], ['dog'], ['bird']])\n",
    "encoded_data = encoder.fit_transform(categories).toarray()\n",
    "\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "print(\"Encoded Data:\\n\", encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ***What is a Test Set?***\n",
    "*Answer-*\n",
    "\n",
    "A **test set** is a subset of the dataset that is reserved for evaluating the performance of a machine learning model. Unlike the training set, which is used to train the model, the test set is never seen by the model during the training process. It acts as unseen data to measure how well the model generalizes to new data.\n",
    "\n",
    "### Why is a Test Set Important?\n",
    "1. **Generalization Performance:** The test set evaluates how well the model performs on unseen data, ensuring that the model does not overfit or underfit the training data.\n",
    "2. **Model Comparison:** It helps compare different models or configurations to determine which performs best on unseen data.\n",
    "3. **Bias Detection:** It highlights potential biases or shortcomings in the model, such as overfitting to the training data.\n",
    "\n",
    "### Key Characteristics of a Test Set:\n",
    "1. **Independence:** The test set must be independent of the training data to ensure an unbiased evaluation.\n",
    "2. **Proportion:** Typically, the test set constitutes 20–30% of the total dataset, depending on the size of the dataset.\n",
    "3. **Fixed During Evaluation:** The test set remains unchanged during model training and hyperparameter tuning to avoid data leakage.\n",
    "\n",
    "### How is the Test Set Used?\n",
    "After training a model on the training data, predictions are generated for the test set. The predictions are compared to the true labels (ground truth) using performance metrics, such as accuracy, precision, recall, F1-score, or Mean Squared Error (MSE), depending on the type of problem (classification or regression).\n",
    "\n",
    "### Difference Between Training, Validation, and Test Sets:\n",
    "1. **Training Set:** Used to fit the model and learn the parameters (e.g., weights).\n",
    "2. **Validation Set:** Used during training for hyperparameter tuning and model selection.\n",
    "3. **Test Set:** Used after training and validation to evaluate the final model's performance.\n",
    "\n",
    "### Conclusion:\n",
    "The test set is crucial for assessing a machine learning model's ability to generalize to new, unseen data. Proper usage ensures a robust evaluation of the model and helps detect issues such as overfitting or data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Predictions: [2.14285714]\n",
      "Mean Squared Error: 1.8418367346938764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]  # Features\n",
    "y = [1.5, 3.5, 2.0, 5.0, 4.5]  # Target values\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Predictions:\", y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ***How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem ?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "To split a dataset into training and testing subsets in Python, we use the `train_test_split` function from the Scikit-learn library. This function divides the data into two parts: one used for training the model and the other for testing its performance.\n",
    "\n",
    "#### Steps for Splitting Data:\n",
    "\n",
    "1. **Import the Required Module:**\n",
    "   Import the `train_test_split` function from `sklearn.model_selection`.\n",
    "\n",
    "2. **Prepare the Data:**\n",
    "   Your dataset typically consists of:\n",
    "   - `X` (features): The independent variables.\n",
    "   - `y` (target): The dependent variable or labels.\n",
    "\n",
    "3. **Use `train_test_split`:**\n",
    "   Use the function to divide the data into training and testing sets, specifying the `test_size` (proportion of the data for testing) and `random_state` (to ensure reproducibility).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### How Do You Approach a Machine Learning Problem?\n",
    "\n",
    "When solving a machine learning problem, it is essential to follow a structured approach to ensure accuracy, efficiency, and reproducibility. Below is a step-by-step guide:\n",
    "\n",
    "#### 1. **Understand the Problem:**\n",
    "   - Clearly define the objective (e.g., classification, regression, clustering).\n",
    "   - Identify the target variable and the desired output.\n",
    "   - Understand the business or research context to ensure your solution aligns with the goals.\n",
    "\n",
    "#### 2. **Collect and Explore Data:**\n",
    "   - Gather the dataset from relevant sources (e.g., databases, APIs, files).\n",
    "   - Perform **Exploratory Data Analysis (EDA)**:\n",
    "     - Visualize data distribution, trends, and relationships.\n",
    "     - Identify missing values, outliers, and errors.\n",
    "\n",
    "#### 3. **Preprocess the Data:**\n",
    "   - Handle missing data (e.g., using imputation techniques).\n",
    "   - Encode categorical variables (e.g., one-hot encoding or label encoding).\n",
    "   - Perform feature scaling if required (e.g., normalization or standardization).\n",
    "\n",
    "#### 4. **Split the Data:**\n",
    "   - Divide the dataset into training and testing subsets (and validation if necessary).\n",
    "   - Ensure the split is representative of the entire dataset.\n",
    "\n",
    "#### 5. **Select a Model:**\n",
    "   - Choose an appropriate machine learning algorithm based on the problem type and data (e.g., logistic regression for binary classification, decision trees for interpretable models).\n",
    "\n",
    "#### 6. **Train the Model:**\n",
    "   - Fit the model on the training dataset.\n",
    "   - Use hyperparameter tuning (e.g., Grid Search or Random Search) if required.\n",
    "\n",
    "#### 7. **Evaluate the Model:**\n",
    "   - Assess model performance on the test set using relevant metrics:\n",
    "     - Classification: Accuracy, precision, recall, F1-score, ROC-AUC.\n",
    "     - Regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), R² score.\n",
    "   - Analyze whether the model is overfitting or underfitting.\n",
    "\n",
    "#### 8. **Optimize the Model:**\n",
    "   - Experiment with feature selection, engineering, or algorithm parameters to improve performance.\n",
    "   - Use cross-validation to ensure robustness.\n",
    "\n",
    "#### 9. **Deploy and Monitor:**\n",
    "   - Deploy the model into production or deliver it as a solution.\n",
    "   - Monitor its performance in real-world scenarios and update as needed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Workflow:\n",
    "Example of approaching a supervised classification problem:\n",
    "\n",
    "1. Problem: Predict whether a customer will churn (leave) based on their behavior.\n",
    "2. Data: Customer demographics, usage patterns, and subscription history.\n",
    "3. Preprocessing: Encode categorical data (e.g., `gender`) and scale numerical data.\n",
    "4. Model: Train a Logistic Regression model.\n",
    "5. Evaluation: Use accuracy and F1-score to evaluate predictions.\n",
    "6. Deployment: Integrate the model into the company’s CRM system for real-time predictions.\n",
    "\n",
    "By following these structured steps, we can systematically build and evaluate machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " [[ 9 10]\n",
      " [ 5  6]\n",
      " [ 1  2]\n",
      " [ 7  8]]\n",
      "Testing Features:\n",
      " [[3 4]]\n",
      "Training Labels:\n",
      " [0 0 0 1]\n",
      "Testing Labels:\n",
      " [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#### Key Parameters in `train_test_split`:\\n- `test_size`: Proportion of the dataset to include in the test split (e.g., `0.2` for 20% testing data).\\n- `train_size`: Proportion of the dataset to include in the training split (complementary to `test_size` if not specified).\\n- `random_state`: Ensures the split is reproducible (e.g., using `random_state=42`).\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset >>\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features\n",
    "y = np.array([0, 1, 0, 1, 0])  # Target labels\n",
    "\n",
    "# Splitting the data >>\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n",
    "print(\"Training Labels:\\n\", y_train)\n",
    "print(\"Testing Labels:\\n\", y_test)\n",
    "\n",
    "\"\"\"#### Key Parameters in `train_test_split`:\n",
    "- `test_size`: Proportion of the dataset to include in the test split (e.g., `0.2` for 20% testing data).\n",
    "- `train_size`: Proportion of the dataset to include in the training split (complementary to `test_size` if not specified).\n",
    "- `random_state`: Ensures the split is reproducible (e.g., using `random_state=42`).\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ***Why do we have to perform EDA before fitting a model to the data?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is a critical step in any machine learning workflow. It involves analyzing and summarizing the data to understand its structure, uncover patterns, and identify potential problems before building a model. Without EDA, the model-building process is less informed and more prone to errors.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Understand the Data Distribution**\n",
    "   - EDA helps you understand the basic properties of your dataset, such as the range, mean, median, variance, and distribution of numerical variables.\n",
    "   - Example:\n",
    "     - Visualizing a histogram can reveal whether a feature is normally distributed, skewed, or has outliers.\n",
    "   - Why it’s important:\n",
    "     - Many machine learning algorithms assume certain distributions (e.g., normal distribution in linear regression), and EDA ensures you can preprocess the data accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Detect Missing Values**\n",
    "   - EDA identifies missing or incomplete data in your dataset.\n",
    "   - Techniques:\n",
    "     - Use `isnull()` or `info()` functions in Python to identify missing values.\n",
    "   - Why it’s important:\n",
    "     - Missing values can cause errors or biases in model training if not handled (e.g., imputation or deletion).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Identify Outliers**\n",
    "   - Outliers are extreme values that deviate significantly from the rest of the data and can skew model results.\n",
    "   - Techniques:\n",
    "     - Use box plots or scatter plots to detect outliers.\n",
    "   - Why it’s important:\n",
    "     - Certain algorithms (e.g., linear regression) are sensitive to outliers, and they need to be treated (e.g., removed or capped).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Feature Relationships and Dependencies**\n",
    "   - EDA helps uncover relationships between features and the target variable, such as:\n",
    "     - Correlations: How two variables are related.\n",
    "     - Trends: Patterns over time or categories.\n",
    "   - Techniques:\n",
    "     - Use correlation matrices, scatter plots, or pair plots.\n",
    "   - Why it’s important:\n",
    "     - Identifying important features helps improve model performance and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Detect Data Imbalance**\n",
    "   - EDA helps identify imbalanced datasets, especially in classification problems.\n",
    "     - Example: Fraud detection datasets may have 95% non-fraud cases and 5% fraud cases.\n",
    "   - Why it’s important:\n",
    "     - Imbalanced data can bias the model toward the majority class, requiring techniques like oversampling or undersampling.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Validate Assumptions**\n",
    "   - Many machine learning algorithms have underlying assumptions (e.g., linearity, normality, independence).\n",
    "   - Techniques:\n",
    "     - Plot residuals or use statistical tests (e.g., Shapiro-Wilk test for normality).\n",
    "   - Why it’s important:\n",
    "     - EDA ensures these assumptions are met or highlights the need for transformations (e.g., log transformation).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Feature Engineering and Selection**\n",
    "   - EDA helps identify:\n",
    "     - Redundant features: Highly correlated features that can be removed.\n",
    "     - New features: Opportunities to create meaningful derived variables.\n",
    "   - Why it’s important:\n",
    "     - Good features significantly improve model performance and reduce computational complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Detect Data Leakage**\n",
    "   - EDA can uncover data leakage, where information from the target variable is inadvertently included in the features.\n",
    "   - Why it’s important:\n",
    "     - Data leakage can artificially inflate model performance and lead to poor real-world predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Improve Model Performance**\n",
    "   - EDA identifies necessary preprocessing steps (e.g., scaling, encoding), ensuring the data is ready for the model.\n",
    "   - Why it’s important:\n",
    "     - Properly prepared data leads to better training results and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Understand Domain Context**\n",
    "   - EDA bridges the gap between raw data and domain knowledge, ensuring the model aligns with real-world expectations.\n",
    "   - Example:\n",
    "     - A healthcare dataset might require domain-specific feature transformations or categorizations.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Performing EDA is an essential step to ensure data quality, reveal insights, and prepare the data for modeling. Skipping EDA can lead to poorly performing models, unreliable results, and misinterpretations of the data. By investing time in EDA, we set a solid foundation for the rest of the machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ***How Can You Find Correlation Between Variables in Python?***\n",
    "*Answer-*\n",
    "\n",
    "To find the correlation between variables in Python, we can use libraries like **Pandas**, **NumPy**, or visualization tools like **Seaborn**. Correlation measures how two variables are related, with values ranging between -1 and 1:\n",
    "- **+1:** Perfect positive correlation.\n",
    "- **0:** No correlation.\n",
    "- **-1:** Perfect negative correlation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Using Pandas' `corr()` Method**\n",
    "Pandas provides the `.corr()` method to compute pairwise correlation between numerical variables in a DataFrame.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'salary': [50000, 60000, 70000, 80000, 90000],\n",
    "    'experience': [1, 3, 5, 7, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "               age   salary  experience\n",
    "age           1.0      1.0        1.0\n",
    "salary        1.0      1.0        1.0\n",
    "experience    1.0      1.0        1.0\n",
    "```\n",
    "\n",
    "- **Interpretation:**\n",
    "  - A correlation of `1.0` indicates a perfect positive correlation among the variables in this example.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Visualizing Correlation Using Seaborn's Heatmap**\n",
    "A heatmap makes it easier to visualize correlations.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "A heatmap is displayed where:\n",
    "- **Cells close to red indicate strong positive correlation.**\n",
    "- **Cells close to blue indicate strong negative correlation.**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Using NumPy's `corrcoef()` Function**\n",
    "NumPy provides the `corrcoef()` function to compute correlation between arrays.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample arrays\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Correlation coefficient\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(correlation)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "[[1. 1.]\n",
    " [1. 1.]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Computing Correlation for Specific Columns**\n",
    "We can compute the correlation between two specific columns using Pandas.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "# Correlation between 'age' and 'salary'\n",
    "correlation_value = df['age'].corr(df['salary'])\n",
    "print(f\"Correlation between age and salary: {correlation_value}\")\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "Correlation between age and salary: 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Specifying Correlation Methods**\n",
    "By default, Pandas uses the Pearson correlation. can also specify:\n",
    "- **`method='pearson'`** (default): Linear correlation.\n",
    "- **`method='spearman'`**: Rank-based correlation.\n",
    "- **`method='kendall'`**: Correlation based on concordant and discordant pairs.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "# Compute Spearman correlation\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "print(spearman_corr)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Handling Non-Numerical Variables**\n",
    "Non-numerical variables (categorical data) need to be encoded before computing correlation:\n",
    "- Use **Label Encoding** or **One-Hot Encoding** to convert categorical data into numerical format.\n",
    "\n",
    "#### Example of Encoding:\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {'gender': ['Male', 'Female', 'Female', 'Male', 'Male']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "df['gender_encoded'] = encoder.fit_transform(df['gender'])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Using Scipy's `pearsonr` or `spearmanr`**\n",
    "The SciPy library provides statistical methods for correlation.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Sample data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [5, 6, 7, 8, 9]\n",
    "\n",
    "# Pearson correlation and p-value\n",
    "corr, p_value = pearsonr(x, y)\n",
    "print(f\"Pearson Correlation: {corr}, P-value: {p_value}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Python offers versatile methods for finding correlations between variables, from numerical computations to visualizations. The choice of method depends on the data and the type of analysis required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. ***What is causation? Explain difference between correlation and causation with an example.***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "Causation refers to a cause-and-effect relationship between two variables, where one variable directly influences or brings about a change in the other. In other words, causation means **\"A causes B\"**.\n",
    "\n",
    "For example:\n",
    "- Increased advertising spending (A) causes higher product sales (B).\n",
    "- Lack of exercise (A) causes weight gain (B).\n",
    "\n",
    "---\n",
    "\n",
    "### Difference Between Correlation and Causation\n",
    "\n",
    "**Correlation** and **causation** are often confused, but they are fundamentally different concepts:\n",
    "\n",
    "| **Aspect**               | **Correlation**                                                                 | **Causation**                                   |\n",
    "|---------------------------|----------------------------------------------------------------------------------|------------------------------------------------|\n",
    "| **Definition**            | A statistical relationship or association between two variables.                | A direct cause-and-effect relationship between variables. |\n",
    "| **Direction**             | No implied direction of influence between variables.                            | One variable directly impacts the other.       |\n",
    "| **Evidence**              | Indicates a relationship exists but does not prove one causes the other.        | Establishes that one variable is responsible for the change in the other. |\n",
    "| **Example**               | Ice cream sales and drowning rates are correlated.                              | Smoking causes lung cancer.                    |\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Correlation vs. Causation\n",
    "\n",
    "#### **Correlation Example**\n",
    "- **Scenario:** Data shows that as ice cream sales increase, drowning incidents also increase.\n",
    "- **Interpretation:** Ice cream sales and drowning incidents are positively correlated.\n",
    "- **Reality:** The correlation exists because both variables increase during summer months, but one does not cause the other.\n",
    "\n",
    "#### **Causation Example**\n",
    "- **Scenario:** Research shows that regular exercise reduces body weight.\n",
    "- **Interpretation:** Exercise causes weight loss because it burns calories and boosts metabolism.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "1. **Correlation ≠ Causation:** A correlation between two variables does not mean one causes the other.\n",
    "2. **Confounding Factors:** Sometimes, a third variable influences both correlated variables. For instance:\n",
    "   - **Example:** Hot weather increases both ice cream sales and swimming activities, which leads to more drowning incidents.\n",
    "3. **Establishing Causation:** To prove causation, controlled experiments or statistical techniques like causal inference are needed.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Establish Causation?\n",
    "To determine causation, we can use:\n",
    "1. **Randomized Controlled Trials (RCTs):**\n",
    "   - Example: A drug trial to see if a medication causes a decrease in blood pressure.\n",
    "2. **Statistical Tests:**\n",
    "   - Tools like regression analysis with control variables.\n",
    "3. **Granger Causality:**\n",
    "   - Determines whether one time series predicts another.\n",
    "4. **Directed Acyclic Graphs (DAGs):**\n",
    "   - Used to model causal relationships.\n",
    "\n",
    "By carefully analyzing the data and ruling out confounding factors, causation can be identified with greater certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. ***What is an Optimizer? What are different types of optimizers? Explain each with an example.***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "An **optimizer** in machine learning is an algorithm used to adjust the parameters (weights and biases) of a model to minimize the **loss function**. The loss function measures the error between the model's predictions and the actual target values. Optimizers play a crucial role in training models by improving performance and helping them converge to an optimal solution.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Optimizers\n",
    "\n",
    "There are several types of optimizers commonly used in machine learning, especially for neural networks. Here's a detailed explanation of the most popular ones:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Gradient Descent**\n",
    "\n",
    "**Description:**\n",
    "- Gradient Descent is a fundamental optimization algorithm that minimizes the loss function by iteratively updating model parameters in the direction of the negative gradient (steepest descent).\n",
    "\n",
    "\n",
    "**Types:**\n",
    "- **Batch Gradient Descent:** Updates parameters using the entire dataset.\n",
    "- **Advantages:** Stable convergence.\n",
    "- **Disadvantages:** Computationally expensive for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**Description:**\n",
    "- In Stochastic Gradient Descent, parameters are updated using one data point (sample) at a time, rather than the entire dataset.\n",
    "\n",
    "**Advantages:**\n",
    "- Faster updates and convergence for large datasets.\n",
    "- Suitable for online learning.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Noisy updates may lead to convergence fluctuations.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "**Description:**\n",
    "- A compromise between Batch Gradient Descent and SGD, it updates parameters using a small batch of data points (mini-batch).\n",
    "\n",
    "**Advantages:**\n",
    "- Faster convergence than Batch Gradient Descent.\n",
    "- Reduces noise compared to SGD.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "batch_size = 32\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Momentum**\n",
    "\n",
    "**Description:**\n",
    "- Momentum adds an exponentially weighted average of previous gradients to the current update, helping the optimizer accelerate in relevant directions and dampen oscillations.\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "- Speeds up convergence.\n",
    "- Reduces oscillations.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "**Description:**\n",
    "- RMSprop divides the learning rate by a running average of the magnitudes of recent gradients, ensuring that the updates are not too large.\n",
    "\n",
    "**Advantages:**\n",
    "- Works well for non-stationary objectives.\n",
    "- Suitable for training deep neural networks.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "**Description:**\n",
    "- Adam combines the benefits of Momentum and RMSprop by maintaining both an exponentially decaying average of past gradients and their squares.\n",
    "\n",
    "**Advantages:**\n",
    "- Fast convergence.\n",
    "- Suitable for sparse data and large datasets.\n",
    "\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **AdaGrad (Adaptive Gradient Algorithm)**\n",
    "\n",
    "**Description:**\n",
    "- AdaGrad adapts the learning rate for each parameter, ensuring that infrequently updated parameters receive larger updates.\n",
    "\n",
    "**Advantages:**\n",
    "- Works well for sparse datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Learning rate may shrink too much over time.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **AdaDelta**\n",
    "\n",
    "**Description:**\n",
    "- An improvement over AdaGrad, AdaDelta restricts the window of accumulated past gradients to prevent the learning rate from decaying too much.\n",
    "\n",
    "**Advantages:**\n",
    "- Overcomes the diminishing learning rate issue of AdaGrad.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison of Optimizers\n",
    "\n",
    "| **Optimizer**      | **Speed of Convergence** | **Robustness**       | **Common Use Case**                     |\n",
    "|---------------------|--------------------------|----------------------|------------------------------------------|\n",
    "| Gradient Descent    | Slow                    | Stable               | Simple models, small datasets.           |\n",
    "| SGD                 | Fast                    | Noisy                | Large datasets, online learning.         |\n",
    "| Momentum            | Faster than SGD         | Less oscillation     | Training deep neural networks.           |\n",
    "| RMSprop             | Fast                    | Effective for deep learning | RNNs and other deep learning models. |\n",
    "| Adam                | Very fast               | Versatile            | Most deep learning models.               |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Choosing the right optimizer depends on the model type, dataset size, and computational resources. **Adam** is often a good default choice due to its balance of speed and performance. However, experimenting with different optimizers can sometimes yield better results for specific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. ***What is sklearn.linear_model?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "`sklearn.linear_model` is a module in the Scikit-learn library that provides implementations of various linear models for regression and classification tasks. These models are designed to predict a target variable based on a linear combination of the input features.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Models in `sklearn.linear_model`\n",
    "\n",
    "#### 1. **Linear Regression**\n",
    "- **Purpose:** Fits a linear relationship between input features (X) and target values (y).\n",
    "- **Use Case:** Predicting continuous variables (e.g., house prices, stock values).\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Logistic Regression**\n",
    "- **Purpose:** A classification algorithm that predicts probabilities for binary or multiclass targets using a logistic function.\n",
    "- **Use Case:** Spam email detection, medical diagnosis.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Ridge Regression**\n",
    "- **Purpose:** A regularized version of linear regression that adds an L2 penalty to reduce overfitting.\n",
    "- **Use Case:** When the dataset has multicollinearity or high-dimensional data.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize model\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Lasso Regression**\n",
    "- **Purpose:** Adds an L1 penalty to linear regression, which can shrink coefficients to zero, effectively performing feature selection.\n",
    "- **Use Case:** Sparse datasets or when feature selection is required.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize model\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Elastic Net**\n",
    "- **Purpose:** Combines L1 (Lasso) and L2 (Ridge) regularization to balance feature selection and shrinkage.\n",
    "- **Use Case:** When both L1 and L2 regularization are needed.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize model\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **SGDClassifier**\n",
    "- **Purpose:** Implements stochastic gradient descent for classification tasks.\n",
    "- **Use Case:** Large datasets or when training needs to be incremental.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Initialize model\n",
    "model = SGDClassifier()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **SGDRegressor**\n",
    "- **Purpose:** Implements stochastic gradient descent for regression tasks.\n",
    "- **Use Case:** Large datasets or online learning for regression problems.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Initialize model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Perceptron**\n",
    "- **Purpose:** A simple linear classifier for binary classification based on a single-layer neural network.\n",
    "- **Use Case:** Simple binary classification problems.\n",
    "- **Example Code:**\n",
    "```python\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Initialize model\n",
    "model = Perceptron()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Models in `sklearn.linear_model`\n",
    "\n",
    "- **HuberRegressor:** Robust regression that is less sensitive to outliers.\n",
    "- **PassiveAggressiveClassifier:** Online learning algorithm for large-scale datasets.\n",
    "- **OrthogonalMatchingPursuit:** Regression that selects a subset of features.\n",
    "- **BayesianRidge:** Bayesian interpretation of ridge regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of `sklearn.linear_model`\n",
    "\n",
    "| **Model**                  | **Task**       | **Regularization** | **Key Strength**                    |\n",
    "|----------------------------|----------------|---------------------|--------------------------------------|\n",
    "| LinearRegression           | Regression     | None                | Simple, interpretable.              |\n",
    "| LogisticRegression         | Classification | L2 (default)        | Probabilistic output.               |\n",
    "| Ridge                      | Regression     | L2                  | Handles multicollinearity.          |\n",
    "| Lasso                      | Regression     | L1                  | Feature selection.                  |\n",
    "| ElasticNet                 | Regression     | L1 + L2             | Balanced regularization.            |\n",
    "| SGDClassifier/SGDRegressor | Both           | L1, L2, or none     | Large datasets, online learning.    |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The `sklearn.linear_model` module provides versatile tools for regression and classification problems. By choosing the appropriate model and regularization technique, we can handle a wide range of machine learning tasks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. ***What does model.fit() do? What arguments must be given?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "The `model.fit()` method is used to train a machine learning model by finding patterns in the training data. It adjusts the model's internal parameters (like weights and biases) to minimize the error or loss function. The method varies slightly depending on the type of model (e.g., regression, classification, clustering), but the fundamental goal remains the same: to optimize the model based on the provided training data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Tasks Performed by `model.fit()`**\n",
    "\n",
    "1. **Learning Parameters:** Adjusts the model's weights and biases to reduce the difference between predictions and actual values.\n",
    "2. **Optimization:** Minimizes the loss function using an optimization algorithm (e.g., Gradient Descent, Adam).\n",
    "3. **Training Iterations:** Updates parameters iteratively based on the dataset until convergence or a stopping criterion is met.\n",
    "4. **Internal Setup:** Configures the model for further operations, such as making predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Arguments of `model.fit()`**\n",
    "\n",
    "The required arguments for `model.fit()` depend on the type of model. Below are the most common arguments:\n",
    "\n",
    "#### **1. Mandatory Arguments**\n",
    "- **`X_train`:** The input features of the training dataset, typically provided as a NumPy array, Pandas DataFrame, or similar format. Shape: `(n_samples, n_features)`.\n",
    "- **`y_train`:** The target values corresponding to the input features. For classification, these are labels; for regression, these are continuous values. Shape: `(n_samples,)`.\n",
    "\n",
    "#### **2. Optional Arguments**\n",
    "- **`sample_weight`:** (Optional) Weights for each sample, used when some data points are more important than others.\n",
    "- **`epochs`:** (Specific to neural networks) Number of iterations over the dataset.\n",
    "- **`batch_size`:** (Specific to neural networks) Number of samples processed before updating model parameters.\n",
    "- **`callbacks`:** (Specific to neural networks) Functions executed at specific stages of training, like early stopping.\n",
    "- **`verbose`:** Controls the verbosity of output during training (e.g., 0: silent, 1: progress bar).\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples**\n",
    "\n",
    "#### **Linear Regression Example**\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X_train = [[1], [2], [3], [4]]\n",
    "y_train = [2.5, 5, 7.5, 10]\n",
    "\n",
    "# Initialize model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Logistic Regression Example**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training data\n",
    "X_train = [[1, 2], [3, 4], [5, 6]]\n",
    "y_train = [0, 1, 0]\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Neural Network Example**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **`model.fit()`** is the core function to train models in machine learning.\n",
    "- The mandatory arguments are **`X_train`** and **`y_train`**.\n",
    "- Optional arguments vary by the model type, allowing customization for specific tasks like handling weights or defining training configurations.\n",
    "- Once trained using `model.fit()`, the model is ready to make predictions with methods like `model.predict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. ***What does model.predict() do? What arguments must be given?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "The `model.predict()` method generates predictions from a trained machine learning model. It takes input features and uses the model’s learned parameters to output predictions for each sample in the input data. These predictions could represent probabilities, class labels, or continuous values, depending on the type of model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Tasks Performed by `model.predict()`\n",
    "\n",
    "1. **Forward Pass:** Applies the model's learned parameters (e.g., weights and biases) to the input features.\n",
    "2. **Computation of Predictions:** Produces outputs in a format determined by the model:\n",
    "   - Regression models return continuous values.\n",
    "   - Classification models may return class probabilities or labels.\n",
    "3. **No Parameter Update:** Unlike `model.fit()`, this method does not alter the model's parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Arguments of `model.predict()`\n",
    "\n",
    "#### **1. Mandatory Argument**\n",
    "- **`X_test`:** Input features for which predictions are to be made. It should have the same number of features (columns) as the data used for training. Shape: `(n_samples, n_features)`.\n",
    "\n",
    "#### **2. Optional Arguments**\n",
    "- **`batch_size`:** (Specific to neural networks) Number of samples processed at a time.\n",
    "- **`verbose`:** (Specific to neural networks) Controls the verbosity of the prediction process (e.g., 0: silent, 1: progress updates).\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs of `model.predict()`\n",
    "- **Regression Models:** Returns continuous numeric predictions (e.g., house price predictions).\n",
    "- **Classification Models:**\n",
    "  - If returning raw probabilities (e.g., logistic regression), predictions are floating-point numbers between 0 and 1.\n",
    "  - If returning class labels, predictions are integers or strings corresponding to class indices.\n",
    "\n",
    "---\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### **Linear Regression Example**\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X_train = [[1], [2], [3], [4]]\n",
    "y_train = [2.5, 5, 7.5, 10]\n",
    "X_test = [[5], [6]]\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)  # Output: [12.5, 15]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Logistic Regression Example**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample data\n",
    "X_train = [[1, 2], [3, 4], [5, 6]]\n",
    "y_train = [0, 1, 0]\n",
    "X_test = [[2, 3], [4, 5]]\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict class labels\n",
    "class_predictions = model.predict(X_test)\n",
    "print(class_predictions)  # Output: [0, 1]\n",
    "\n",
    "# Predict probabilities\n",
    "prob_predictions = model.predict_proba(X_test)\n",
    "print(prob_predictions)  # Output: [[0.7, 0.3], [0.4, 0.6]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Neural Network Example**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Sample data\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(2, size=100)\n",
    "X_test = np.random.rand(10, 10)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=10, verbose=0)\n",
    "\n",
    "# Predict probabilities\n",
    "prob_predictions = model.predict(X_test)\n",
    "print(prob_predictions)\n",
    "\n",
    "# Predict class labels\n",
    "class_predictions = (prob_predictions > 0.5).astype(int)\n",
    "print(class_predictions)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **`model.predict()`** generates predictions using a trained model.\n",
    "- The mandatory argument is **`X_test`**, which contains the input features.\n",
    "- Outputs depend on the model type:\n",
    "  - **Regression:** Returns continuous values.\n",
    "  - **Classification:** Returns probabilities or class labels.\n",
    "- It's essential to ensure that the input features for prediction have the same structure and preprocessing as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. ***What is feature scaling? How does it help in Machine Learning?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "Feature scaling is a data preprocessing technique that adjusts the range of features (independent variables) in a dataset to a common scale without distorting their relative relationships. It is an essential step in preparing data for machine learning algorithms sensitive to the magnitude of feature values.\n",
    "\n",
    "Common methods for feature scaling include:\n",
    "1. **Normalization:** Scales data to a fixed range, typically [0, 1].\n",
    "2. **Standardization:** Transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does Feature Scaling Help in Machine Learning?\n",
    "\n",
    "Feature scaling improves the performance, accuracy, and efficiency of machine learning models in the following ways:\n",
    "\n",
    "1. **Improves Model Convergence**\n",
    "   - Algorithms like Gradient Descent optimize faster when features have similar scales. Without scaling, features with larger ranges dominate the optimization process, leading to slower convergence.\n",
    "\n",
    "2. **Prevents Dominance of Large-Scale Features**\n",
    "   - Features with larger magnitudes may disproportionately influence the model, biasing the results. Scaling ensures each feature contributes equally to the model.\n",
    "\n",
    "3. **Enhances Distance-Based Metrics**\n",
    "   - Models like k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and clustering algorithms (e.g., k-Means) rely on distance metrics. Unequal feature scales can skew distance calculations.\n",
    "\n",
    "4. **Improves Numerical Stability**\n",
    "   - Some algorithms, such as Logistic Regression or Neural Networks, are sensitive to numerical instability caused by large or widely varying feature values. Scaling mitigates this issue.\n",
    "\n",
    "5. **Ensures Consistency Across Features**\n",
    "   - Scaling aligns features into a uniform range, making the model’s coefficients more interpretable, especially in linear models.\n",
    "\n",
    "---\n",
    "\n",
    "### When Is Feature Scaling Necessary?\n",
    "\n",
    "Feature scaling is crucial for:\n",
    "1. **Distance-based algorithms:** k-NN, k-Means, DBSCAN, etc.\n",
    "2. **Gradient-based algorithms:** Logistic Regression, Neural Networks, SVMs.\n",
    "3. **Principal Component Analysis (PCA):** Ensures correct computation of principal components.\n",
    "\n",
    "It is generally unnecessary for algorithms like Decision Trees and Random Forests, which are not sensitive to feature magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Methods for Feature Scaling\n",
    "\n",
    "1. **Normalization (Min-Max Scaling)**\n",
    "   - Scales each feature to a fixed range, typically [0, 1].\n",
    "   - Example:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "2. **Standardization (Z-Score Scaling)**\n",
    "   - Transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "     scaler = StandardScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "3. **Robust Scaling**\n",
    "   - Uses the median and interquartile range, making it robust to outliers.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import RobustScaler\n",
    "     scaler = RobustScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Importance of Feature Scaling\n",
    "\n",
    "#### Without Scaling:\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Features with different scales\n",
    "X = np.array([[1, 1000], [2, 2000], [3, 3000]])\n",
    "y = [0, 1, 0]\n",
    "\n",
    "# Train SVM without scaling\n",
    "model = SVC()\n",
    "model.fit(X, y)\n",
    "```\n",
    "The larger scale of the second feature dominates the training process, leading to poor model performance.\n",
    "\n",
    "#### With Scaling:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train SVM with scaled data\n",
    "model.fit(X_scaled, y)\n",
    "```\n",
    "By scaling the features, both contribute equally, resulting in better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Feature scaling ensures that features contribute proportionally to model training and prediction. It improves convergence speed, enhances model performance for distance-based algorithms, and prevents numerical instability. Proper scaling is a critical step in preprocessing data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Features with different scales\n",
    "X = np.array([[1, 1000], [2, 2000], [3, 3000]])\n",
    "y = [0, 1, 0]\n",
    "\n",
    "# Train SVM without scaling\n",
    "model = SVC()\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train SVM with scaled data\n",
    "model.fit(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. ***How do we perform scaling in Python?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "Feature scaling in Python is typically done using libraries like **Scikit-learn**, which provides several tools for scaling and normalizing data efficiently. Below are the most commonly used methods for scaling, along with code examples.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Normalization (Min-Max Scaling)**\n",
    "\n",
    "Normalization scales the data to a fixed range, usually \\([0, 1]\\). It’s useful when the distribution of data is not Gaussian or when features have different ranges.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\\[\n",
    "\\text{Scaled Data: }\n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "1.0 & 1.0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Standardization (Z-Score Scaling)**\n",
    "\n",
    "Standardization transforms data to have a mean of 0 and a standard deviation of 1. It’s commonly used when data follows a Gaussian distribution.\n",
    "\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "The features are transformed to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Robust Scaling**\n",
    "\n",
    "Robust Scaling uses the **median** and **interquartile range (IQR)**, making it robust to outliers.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with outliers\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500], [100, 2000]])\n",
    "\n",
    "# Initialize RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "The outlier has less influence due to the robust scaling method.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Scaling for a Single Feature**\n",
    "\n",
    "We can scale a single feature (or column) independently:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Single feature (column)\n",
    "X = [[500], [1000], [1500]]\n",
    "\n",
    "# Scale single column\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Manual Scaling**\n",
    "\n",
    "If  want to scale manually without libraries:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Normalize manually (Min-Max Scaling)\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_normalized = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "print(X_normalized)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "1. **Fit and Transform Separately for Train/Test Data:**\n",
    "   Always fit the scaler on the training data and then transform both training and testing datasets to avoid data leakage:\n",
    "   ```python\n",
    "   scaler = StandardScaler()\n",
    "   X_train_scaled = scaler.fit_transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "   ```\n",
    "\n",
    "2. **Choice of Scaling Method:**\n",
    "   - Use **Normalization** for bounded data (e.g., pixel intensities).\n",
    "   - Use **Standardization** for unbounded and Gaussian-distributed data.\n",
    "   - Use **Robust Scaling** when the data contains outliers.\n",
    "\n",
    "3. **Avoid Scaling Target Variable:**\n",
    "   Scaling is typically applied only to input features, not the target variable (e.g., in regression).\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Python's Scikit-learn library provides powerful tools like `MinMaxScaler`, `StandardScaler`, and `RobustScaler` for efficient feature scaling. Select the appropriate scaling method based on the data characteristics and the requirements of the machine learning algorithm you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05882353 -1.        ]\n",
      " [-0.01960784 -0.33333333]\n",
      " [ 0.01960784  0.33333333]\n",
      " [ 3.82352941  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with outliers\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500], [100, 2000]])\n",
    "\n",
    "# Initialize RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. ]\n",
      " [0.5]\n",
      " [1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Single feature (column)\n",
    "X = [[500], [1000], [1500]]\n",
    "\n",
    "# Scale single column\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 500], [2, 1000], [3, 1500]])\n",
    "\n",
    "# Normalize manually (Min-Max Scaling)\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_normalized = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "print(X_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. ***Explain Data Enconding.***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "Data encoding is the process of converting categorical variables (non-numeric data) into a numeric format that can be used by machine learning algorithms. Most machine learning algorithms require numerical inputs to process the data effectively. Encoding ensures that the data is represented in a way that the algorithms can interpret.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Data Encoding Important?\n",
    "\n",
    "1. **Machine Learning Compatibility:** Algorithms like Support Vector Machines (SVMs), Linear Regression, and Neural Networks require numerical inputs.\n",
    "2. **Preserves Information:** Proper encoding retains the relationships and hierarchy between categories when applicable.\n",
    "3. **Improves Performance:** Encoded data often leads to better performance and accuracy of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Data Encoding\n",
    "\n",
    "#### 1. **Label Encoding**\n",
    "- Assigns a unique integer to each category.\n",
    "- Suitable for ordinal categorical variables where order matters.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encode data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)  # Output: [1, 2, 0, 2, 1]\n",
    "```\n",
    "\n",
    "**Limitation:** For nominal data (no order), label encoding may introduce unintended ordinal relationships.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **One-Hot Encoding**\n",
    "- Creates binary columns for each category.\n",
    "- Suitable for nominal categorical variables where order does not matter.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array(['Red', 'Blue', 'Green', 'Blue'])\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Encode data\n",
    "encoded_data = encoder.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "print(encoded_data)\n",
    "# Output:\n",
    "# [[0. 0. 1.]\n",
    "#  [1. 0. 0.]\n",
    "#  [0. 1. 0.]\n",
    "#  [1. 0. 0.]]\n",
    "```\n",
    "\n",
    "**Limitation:** Can increase dimensionality significantly if the number of categories is large.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Ordinal Encoding**\n",
    "- Similar to Label Encoding but respects the order of categories.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Quality': ['Low', 'Medium', 'High', 'Medium', 'Low']})\n",
    "\n",
    "# Define mapping for ordinal categories\n",
    "quality_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "\n",
    "# Apply mapping\n",
    "data['Quality_Encoded'] = data['Quality'].map(quality_mapping)\n",
    "\n",
    "print(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Binary Encoding**\n",
    "- Combines aspects of One-Hot Encoding and Label Encoding. Each category is first label-encoded, and then converted to binary.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from category_encoders import BinaryEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue']})\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "encoder = BinaryEncoder()\n",
    "\n",
    "# Encode data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Frequency Encoding**\n",
    "- Replaces each category with its frequency count or proportion in the dataset.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Red', 'Green', 'Blue']})\n",
    "\n",
    "# Frequency encoding\n",
    "frequency = data['Color'].value_counts()\n",
    "data['Color_Encoded'] = data['Color'].map(frequency)\n",
    "\n",
    "print(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Target Encoding**\n",
    "- Replaces categories with the mean of the target variable for each category.\n",
    "- Useful in regression problems.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Category': ['A', 'B', 'A', 'C', 'B'], 'Target': [1, 0, 1, 0, 1]})\n",
    "\n",
    "# Calculate target mean for each category\n",
    "target_mean = data.groupby('Category')['Target'].mean()\n",
    "data['Category_Encoded'] = data['Category'].map(target_mean)\n",
    "\n",
    "print(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing the Right Encoding Method\n",
    "\n",
    "1. **Nominal Data (No Order):**\n",
    "   - One-Hot Encoding or Binary Encoding.\n",
    "2. **Ordinal Data (Ordered):**\n",
    "   - Label Encoding or Ordinal Encoding.\n",
    "3. **High Cardinality:**\n",
    "   - Binary Encoding, Target Encoding, or Frequency Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Use Case\n",
    "\n",
    "#### Dataset:\n",
    "| Color  | Size  | Target |\n",
    "|--------|-------|--------|\n",
    "| Red    | Small | 1      |\n",
    "| Blue   | Medium| 0      |\n",
    "| Green  | Large | 1      |\n",
    "\n",
    "#### Encoding:\n",
    "1. **One-Hot Encoding for `Color`:**\n",
    "   - Red → [1, 0, 0], Blue → [0, 1, 0], Green → [0, 0, 1].\n",
    "2. **Ordinal Encoding for `Size`:**\n",
    "   - Small → 1, Medium → 2, Large → 3.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Data encoding is a critical preprocessing step for handling categorical variables in machine learning. The choice of encoding method depends on the type of categorical variable (nominal or ordinal) and the characteristics of the dataset, such as the number of unique categories. Proper encoding ensures the data is ready for machine learning models to process effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quality  Quality_Encoded\n",
      "0     Low                1\n",
      "1  Medium                2\n",
      "2    High                3\n",
      "3  Medium                2\n",
      "4     Low                1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Quality': ['Low', 'Medium', 'High', 'Medium', 'Low']})\n",
    "\n",
    "# Define mapping for ordinal categories\n",
    "quality_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "\n",
    "# Apply mapping\n",
    "data['Quality_Encoded'] = data['Quality'].map(quality_mapping)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_0  Color_1\n",
      "0        0        1\n",
      "1        1        0\n",
      "2        1        1\n",
      "3        1        0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue']})\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "encoder = BinaryEncoder()\n",
    "\n",
    "# Encode data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Color_Encoded\n",
      "0    Red              2\n",
      "1   Blue              2\n",
      "2    Red              2\n",
      "3  Green              1\n",
      "4   Blue              2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Red', 'Green', 'Blue']})\n",
    "\n",
    "# Frequency encoding\n",
    "frequency = data['Color'].value_counts()\n",
    "data['Color_Encoded'] = data['Color'].map(frequency)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Target  Category_Encoded\n",
      "0        A       1               1.0\n",
      "1        B       0               0.5\n",
      "2        A       1               1.0\n",
      "3        C       0               0.0\n",
      "4        B       1               0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Category': ['A', 'B', 'A', 'C', 'B'], 'Target': [1, 0, 1, 0, 1]})\n",
    "\n",
    "# Calculate target mean for each category\n",
    "target_mean = data.groupby('Category')['Target'].mean()\n",
    "data['Category_Encoded'] = data['Category'].map(target_mean)\n",
    "\n",
    "print(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
